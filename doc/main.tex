\documentclass[11pt]{article}

\usepackage{fullpage}

\input{commands}

\newcommand{\mysection}[1]{\vspace{3mm}\noindent\textbf{#1} \\}
\DeclareMathOperator*{\argmax}{\mathsf{argmax}}

\title{Notes}

\begin{document}

\date{}

\maketitle

\vspace{-2cm}


\section{Model decision over FHE}

Problem: Two parties have private data and want to learn the result of a function on their data.

\noindent Example use case:  A hospital  has some private medical records of patients and a client wants to learn whether he is likely to get some disease or to get some good treatment from a doctor based on that data. Neither the hospital nor the client want to share their private data.

Discussion:
We realized that using FHE for running the machine learning algorithm is not that useful because the hospital can learn on plaintext data. Instead, what is more interesting is to hide the resulting learned model from the patient (another example is Google flu trends because Google does not want to release its parameters). So we want to perform FHE on the decision algorithm that uses the model to predict the outcome, and even this was non-trivial to figure out.
Using FHE here makes sense because we are going to use it for security benefits rather than performance benefits.


\subsection{Our solution for hyperplane decision surfaces}
\label{sub:hyperplane_decision}      

\newcommand{\res}{\mathsf{res}}

We assume a class of classification models parameterized by a hyperplane $w$
for which the decision function can be expressed as:
\begin{equation*}
  \hat{y}(x) = \text{sgn}(\langle w, x \rangle) 
\end{equation*}

In other words, the server has a vector $w$ (the model) and the client has a
vector $x$ (a particular feature vector). The client wants to know whether
$\langle x, w \rangle > 0$, where $\langle \cdot, \cdot \rangle$ denotes dot product. For
now, we assume that $x$ has a finite dimensional basis (this excludes RBF
surfaces, for example).

The client provides $\enc(x)$ to the server, which can easily compute
$\enc(\langle x, w \rangle)$ by only using 1-depth FHE. 

Note that Paillier suffices for this computation. Even though Paillier does not have multiplication homomorphism, we can still implement
multiplication because the server knows $w$ (the model) in plaintext, and can simply raise a ciphertext to the power of $w_i$ and achieve multiplication by $w_i$. Moreover, Paillier has a large plaintext domain so it can certainly fit the ranges of the classification algorithm. 

Let $\res = \langle x,
w \rangle$.

The challenge is deciding if $\res > 0$. Simply giving the client $\enc(\res)$ leaks $\res$ to the client, which is more information about the model than the final decision.

Instead, the idea is for the server to choose a random $\epsilon \in \bbZ_q$ where $q$ is the space of the plaintext in FHE. The server computes $\enc(\epsilon + \res)$ and the client is able to decrypt this ciphertext and obtain $\epsilon + \res \mod q$, which information-theoretically leaks nothing about $\res$.

Now, ideally we would like the client to compare $\epsilon + \res$ to $\epsilon + q/2$, because $\res > q/2$ indicates a negative $\res$ (that wrapped around). We can use Yao's comparison protocol (the millionaires problem -- a simple one interaction protocol) to decide the result of this comparison without leaking any private information. The challenge though is that the client has $\epsilon + \res \mod q$, and $\mod q$ affects correctness.

The idea is to use a new experiment that has a chance of outputting the correct result $>1/2$, and to repeat this protocol a few times and take majority in order to amplify correctness (which increases exponentially in the number of repeats):

\newcommand{\eps}{\epsilon}

\begin{enumerate}
\item
    The server chooses $\epsilon \in \bbZ_q$ at random. If $\epsilon \geq q/2$,
    let $v = \eps - q/2$. The intuition is that an $\eps \geq q/2$ is likely to
    wrap around when added to another number. If $\eps < q/2$, let $v = \eps +
    q/2$.
\item Run Yao's protocol, where the client's input is $\eps + \res \mod q$ and the server's input is $v$.
\end{enumerate}

We now analyze this protocol to show that the probability of success is greater
than half. Let $r \in \bbZ_q$, and $\epsilon \in \bbZ_q$ be drawn uniformly at
random. We also assume that all $r \in \bbZ_q$ are equally likely.
\rnote{We don't need to assume that r is equally likely. Our correctness holds for all $r$. If we assume all r-s are equally likely, we only prove that our protocol succeeds on barely more than half the possible results, but we can in fact show that it should succeed for all possible results with high probability.}
\rbnote{Assuming that the distribution of $r$ is uniform is also problematic as it would basically imply that the distribution of $x$ is uniform (we can discuss about that). The following proof is not correct anymore when we don't suppose that the distribution is uniform: for example, for the second part of the proof ($r \in \bbZ_q^-$), if the distribution is concentrated around $(q-1)/4$ and that the probability of having $(q-1)/2 < r < q-1$ is small, the last probability can be bigger than $1/2$.}

 For now, we
restrict ourselves to odd $q$. Denote $\bbZ_q^+$ as the integers in $[0,
\frac{q-1}{2}]$, and $\bbZ_q^-$ as the integers in $[\frac{q-1}{2}+1, q)$.

\rnote{once you fix an $r$, you should be able to write the probabilities below in terms of $q-r$ or $r$, for the case when $r>q/2$ and $r<q/2$. }
That means $r \in \bbZ_q^+$ we will consider to encode a number $\geq 0$, and
$r \in \bbZ_q^-$ we will consider to encode a number $< 0$. Let $v$ be defined
as before. We want to show that:
\begin{align*}
    \Pr\left[ (r+\epsilon) \text{ mod } q < v\right]
    \begin{cases}
        > 1/2 & \mbox{if } r\in\bbZ_q^+  \\
        < 1/2 & \mbox{if } r\in\bbZ_q^-
    \end{cases}
\end{align*}
We begin by showing that if $r \in \bbZ_q^+$ and $q \geq 7$, then
$\Pr\left[(r+\epsilon)\text{ mod }q<v\right] > 1/2$:
\begin{align*}
    \Pr\left[(r+\epsilon)\text{ mod }q<v : r \in \bbZ_q^+ \right] &=
        \frac{1}{q}\sum\limits_{i=0}^{q-1}\Pr[(r+\epsilon)\text{ mod }q<v : \epsilon=i, r\in \bbZ_q^+ ] \\
        &= \frac{1}{q}\left[ \sum\limits_{i=0}^{(q-1)/2-1} \frac{(q-1)/2}{(q-1)/2+1} + \sum\limits_{i=(q-1)/2}^{q-1} \frac{i-(q-1)/2}{(q-1)/2+1} \right] \\
        &= \frac{1}{4}\left[3+\frac{1}{q}-\frac{8}{1+q}\right]
\end{align*}


\rnote{the probability statements above and below should be over the random choice of $\epsilon$ rather than $r$}

Notice that this quantity is always $> 1/2$ if $q \geq 7$. We now show the other side, that
if $r \in \bbZ_q^-$, then $\Pr\left[(r+\epsilon)\text{ mod }q<v\right] < 1/2$:
\begin{align*}
    \Pr\left[(r+\epsilon)\text{ mod }q<v:r\in\bbZ_q^-\right] &=
        \frac{1}{q}\sum\limits_{i=0}^{q-1}\Pr[(r+\epsilon)\text{ mod }q<v : \epsilon=i,r\in\bbZ_q^-] \\
        &= \frac{1}{q}\left[ \sum\limits_{i=0}^{(q-1)/2-1} \frac{i}{(q-1)/2} + \sum\limits_{i=(q-1)/2}^{q-1} 0 \right] \\
        &= \frac{q-3}{4q}
\end{align*}
It is clear that for $q \geq 3$, this quantity is $< 1/2$. Since we already
require $q \geq 7$ above, this is ok.

\rnote{as a small nit for style, I (and cryptographers) find this notation more clear : $\Pr[\eps \gets \bbZ_q: (r+\eps) \mod q < v]$ because it is clearer where the randomness comes from and what is the order of steps in the experiment.}

\rnote{based on this probability bound, would be good to estimate how many repetitions we need, for example, using Hoeffding's inequality -- see end of Wikipedia page -- or other inequality you prefer }

\rbnote{Hoeffding's inequality requires the random variables to be independent, which is -- I think -- not the case here: remember that the random variable that we consider is the variable that is zero if the guess is correct, and one is incorrect. We could try to draw from the proof of Goldreich-Levin theorem for this point.}

%if r > q/2
 %if e > q/2 : correct result always
 %if e < q/2 for  q - r < e < q/2 incorrect result -- but for e < q-r it is correct
%each case for e happens with chance 1/2

%if r < q/2 :
%if e < q/2 : correct result always
%if e > q/2: correct result sometimes

\subsection{Another solution for rerandomization and comparison}
           
\subsubsection{Description} 
We propose an other protocol for the hyperplane decision surfaces. The idea is very similar to the one propose in section~\ref{sub:hyperplane_decision}.

The server and the client do the same computations as in~\ref{sub:hyperplane_decision} so the server gets an encrypted version of $r$ under the client secret key. The client wants to know the sign of $v$ and we have to ensure server's security \emph{i.e.} the client can only learn the sign of $r$.

\begin{itemize}
	\item The server choose $\varepsilon \getsr \bbZ_q$ and sends the encryption of $r + \varepsilon \mod q$ to the client who decrypts it.
	\item Run Yao's protocol, where the client's input is $r + \varepsilon$ and the server's input is $\varepsilon$.
\end{itemize} 

In this section, when we work modulo $q$, we consider elements in the interval $(-q/2, q/2]$. The client gets the right sign of $r$ if and only if $r + \varepsilon$ does not wrap \emph{i.e.}
\[
	-q/2 < r + \varepsilon \leq q/2 \Leftrightarrow  -q/2 - r< \varepsilon \leq q/2 - r
\]

As a consequence, given $r$, the probability that the protocol is correct is 
\[ 
	\Pr[\varepsilon \getsr \bbZ_q\text{ : Protocol is correct} | r] = \frac{q - |r|}{q} = 1 - \frac{|r|}{q}
\]
and the probability that the client gets the correct sign when $r$ is distributed according to $\mathcal{D}$ is 
\begin{align*}
	\Pr[\varepsilon \getsr \bbZ_q, r \gets \mathcal{D}\text{ : Protocol is correct}] 
		&= \sum_{r \in \mathcal{D}} \Pr[r] (1 - \frac{|r|}{q}) \\
		&= 1 - \frac{1}{q}\sum_{r \in \mathcal{D}} |v| \Pr[v]  \\
		&= 1 - \frac{\mathbb{E}(|\mathcal{D}|)}{q}
\end{align*}
In particular, if $\mathcal{D}$ is bounded by $B$ ($\Pr[ r \gets \mathcal{D} : |r| > B]$), we have
\[
\Pr[\varepsilon \getsr \bbZ_q, r \gets \mathcal{D}\text{ : Protocol is correct}] \geq 1 - \frac{B}{q}
\] 
    
\subsubsection{Repeating the experiment}

We want to repeat the previous protocol $k$ times in order to decrease the probability of a wrong guess: we run the protocol $k$ times and take as an output the majority of outputs. 

Let $X_i$ be the random variable that is 0 if the output of the $i$-th execution of the protocol is correct, and that is 1 otherwise. The final guess is correct if $\sum_{i=1}^k X_i > \frac{k}{2}$. Let $r$ be a possible output of $\mathcal{D}$. For a fixed $r$, it is easy to see that the $X_i$ are independent random variables. Thus, we can apply Chernov's bound:

\[
    \forall \delta > 0,  \Pr\left[ \sum_{i=0}^k X_i \geq (1 + \delta)\mu\right] \leq \left( \frac{e^\delta}{(1+\delta)^{1+\delta}} \right)^\mu
\]
where $\mu = \sum \mathbb{E}(X_i)$. Here, $\mu = k \frac{|r|}{q}$ (remember we fixed $r$). We assumed before that $|r| < q$ and WLOG we can also assume that $|r| < q/2$. And as we want that  $\sum_{i=1}^k X_i > \frac{k}{2}$ with high probability, we can take $\delta = \frac{q}{2|r|} -1 > 0$. We infer that, for $r \neq 0$,
\[
	\Pr[\text{Protocol is not correct} | r] \leq \left(\frac{e^{\frac{q}{2|r|} -1}}{\frac{q}{2|r|}^{\frac{q}{2|r|}}}\right)^{k \frac{|r|}{q}} 
	\leq \left(\frac{2e}{q}\right)^{\frac{k}{2}}
\]
(if $r = 0$, one iteration of the protocol always succeeds as $r + \varepsilon$ does not wrap).


However, repeating the experiment, leaks some informations about $r$: $\mathbb{E}(\sum X_i | v) = k \frac{|r|}{q}$ meaning that the number of incorrect experiments -- and as consequence, if we repeated a sufficient number of times, the number of experiments whose outcome is different from the final outcome -- is about $k \frac{|r|}{q}$. Thus, the more we repeat the protocol, the best we can approximate $|r|$.

\rbnote{We must choose the parameters carefully if we do not want to disclose too many informations about $r$.}

\rbnote{We should also have this issue when we repeat the previous protocol.}

\subsubsection{Bounding the noise} % (fold)
\label{ssub:bounding_the_noise}
	
	In order to avoid overflows when adding noise, one can bound it: if $\mathcal{D}$ is bounded by $B$, we choose $L > 0$ s.t. $L + B \leq q/2$, pick $\varepsilon'$ in $[-L,L]$ and set $\varepsilon = q/2 + \varepsilon'$. Thus, $r + \varepsilon$ does not leak information about $r$ unless $|r + \varepsilon'| < L$, which happens with probability at most $\frac{B}{L}$ (we can choose $L >> B$ to make this probability negligible).
	
	This approach is interesting if we can set $L$ to be a lot smaller than $q$: instead of comparing $\log q$ bits using the millionaires' protocol, we can compare only $\log L$ bits.  
	
% subsubsection bounding_the_noise (end)

\subsection{Na\"ive Bayes solution}

%For na\"ive Bayes, we need to convert the table of values into a matrix so we can FHE multiply it to the input $x$. TODO: we need to figure out how to compute the maximum of $k$ values over FHE, likely using the idea above which computes $\max$ of $q/2$ and $\res$.
%
\newcommand{\NB}{Na\"ive Bayes}
      
For a presentation of the \NB{} classifier, \emph{cf.} \href{http://www.cs.cmu.edu/~tom/mlbook/NBayesLogReg.pdf}{this} ML textbook. For an emphasize on the text classification, see \href{http://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html}{this document}.

We now consider categorial \NB{}. Suppose we have $k$ classes and $n$ features.
Then recall that a \NB{} classifier is parameterized by $\theta_{\mathcal{C}} =
\{\theta_{c}=p(\mathcal{C}=c)\}_{c=1}^{k}$ and $\theta_{\mathcal{F}|c} =
\{\theta_{f|c}= \{ p(\mathcal{F}_f=x|c) \}_{x=1}^{|\mathcal{F}_f|}
\}_{f=1}^{n}$ for each $c = 1,...,k$. The decision function is simply:
\begin{equation*}
  \hat{y}(x) = \argmax_{c} p(c, x) = p(c)\prod\limits_{i=1}^{n}p(x_i|c)
\end{equation*}
To make this easier for FHE eval, we assume that the client supplies us with each feature value
in $1$-of-$k$ encoding, so we receive $n$ ciphertexts $(\enc(x_1), ..., \enc(x_n))$. So now we
can express the decision function as a product of dot products:
\begin{equation*}
  \hat{y}(x) = \argmax_{c} p(x,c) = p(c)\prod\limits_{i=1}^{n} \langle \theta_{\mathcal{F}_i|c}, x_i \rangle
\end{equation*}

      
\subsubsection{First solution} % (fold)
\label{ssub:bayes_solution_1}

Very briefly, here is a solution we have (it has a few rough ends to improve, but it works): It uses the SIMD operations which can actually save us $c$ times computation and storage!

Consider the data at the server in a table with $k$ columns (corresponding to classes -- possible outcomes of the decision) and $N$ rows, the size of domain of feature values. 

The server encrypts each row into an SIMD FHE ciphertext, with a column per slot. 
Let $(x_1, x_2, \dots, x_n)$ be the input to a client.
For each $x_i$, the client sends a vector of $N$ selectors. The $x_i$-th selector is $\fheenc(1|1|1|1)$ and the others are $\fheenc(0|0|0|0)$, where the bits are located in different SIMD slots. Using these ciphertexts, the server can compute $\fheenc(p(x_i, 1)|p(x_i, 2)| \dots | p(x_i, k))$ by multiplying the selectors with the corresponding rows of the server database and then adding up all values for each $x_i$.  If the server multiplies these over all $x_i$, along with appropriate encryptions of $p(c)$, the server obtains:
 $\fheenc(p(x,1)|p(x, 2)| \dots | p(x, k))$.
 
 Now the client and the server need to sort these values to figure out what is the maximum.
 First, the server chooses a random permutation $P$, and permutes the values in the SIMD slots, using SIMD permutations.
 
 Then, the client and the server will run an oblivious sorting network in a non-traditional way. At a very high level, the server will compute encryptions of $\fheenc(p(x, i_1) - p(x, j_1) | p(x, i_2) - p(x, j_2) | \dots )$ using SIMD operations for various stages of the oblivious sorting. Then, the client and the server will run the same protocol we have for the SVM classifier so that the client determines if each difference is positive or negative.  The client does not tell the server the outcome of the differences (based on our previous protocol), but can nevertheless compute which was the maximum entry based on these differences. Say $r$-th entry was the maximum. The client tells $r$ to the server, who answers with the unpermuted $r$, $P^{-1}(r)$, which contains the result of the classification.
   
% subsubsection bayes_solution_1 (end)

\subsubsection{Second solution} % (fold)
\label{ssub:bayes_solution_2}
	Here is an other solution that uses FHE (and can use SIMD operations too).
	
	The server's input are the $kN$ values $\{p(x_j|c_i)\}_{i,j}$ and the $k$ values $\{p(c_i)\}$; the client's input is a subset of feature values.
	
	\begin{enumerate}
		\item Setup: the server generates a key pair for the encryption scheme and publishes the public key. The server encrypts all the input values with this key and publishes them.
		
		\item Using the homomorphic properties of the scheme, the client computes the encryption of the $p(x,c_i)$'s.
		
		\item At this point, the client needs to get the index of the maximum value (the argmax). 
		To do so, it has to do $k$ oblivious comparisons of encrypted values such that only the client learns the result. This will ensure complete privacy of the inputs of both the client and the server and the output (the category with maximum probability) for the client.     
		
		% To do so, the client will run a protocol similar to the one described by Kaghazgaran and Sadeghyan (cf. related work).\\
		% 		The client picks a random value $R$ and adds it to each of the computed values. The client chooses a random permutation $\pi$ over $\{1,...,k\}$ and sends the permuted encryptions to the server.
		% 		\\ \rbnote{There is flaw in the paper: the server can compute the difference between the probabilities and thus learn some informations (which ones??). We can limit the number of differences the server can learn by using a comparison tree.}                                                                    
		% 		
		% 		\item The server decrypts them, compute the argmax and return the result $\tilde{i_0}$ to the client.
		% 		
		% 		\item The client get the argmax $i_0 = \pi^{-1}(\tilde{i_0})$  
	\end{enumerate}
	
	\paragraph{Comparing encrypted data - first solution}
	Suppose Bob has the secret key for an encryption scheme that supports homomorphic addition (FHE, Paillier, ...) and Alice has the encryption of two values $a$ and $b$ ($E(a), E(b)$). Alice wants to know if $a > b$, neither Alice or Bob should learn the value of $a$ or $b$.
	\begin{itemize}
		\item Alice picks a random $r$ and compute $C = E(a).E(b)^{-1}.E(r)$ and sends $C$ to Bob
		
		\item Bob decrypts $C$: $c = D(C)$.
		
		\item Alice and Bob initiate a regular millionaire's to compare $r$ and $c$ in a way that only Alice learns the result (the protocol presented by Lin \& Tzeng - cf. Related Work - satisfies the requirements). 
	\end{itemize} 
	      	
	We can notice here that, if instead of encrypting $p(x_j|c_i)$, we encrypt $\log p(x_j|c_i)$, we replace all the multiplications we need to compute $p(x,c_i)$'s by additions. Thus, we don't really need an FHE scheme, a linearly homomorphic scheme (as Paillier) is sufficient. 
	
	The small size of FHE plaintexts can cause some issues when adding the random noise $R$: the actual maximum can ``overflow'' and thus become the minimum ...
	
	$O(kN)$ communication complexity; $(N+1)k$ encryptions and $k$ decryptions done by the server; $k(n+1)$ homomorphic operations done on the cyphertexts by the client.
	   
	\paragraph{Comparing encrypted data - second solution}
	Use Veugen's result (\emph{cf.} Related Work) but modify it so that Alice (the client) gets the un-encrypted result. 
	  
% subsubsection bayes_solution_2 (end)
          
\subsubsection{Problems to solve} % (fold)
\label{ssub:problems_bayes}
\paragraph{Working with floating point numbers:} We need to find a ``good'' representation of floats for the probabilities, that supports encryption and operations. As the encryption schemes we are using are using integer plaintexts, we have to represent floats as integers. 

The precision of the floats is also an important point as the plaintext space of FHE schemes is small compared to more usual schemes. This is why using logs of probabilities is interesting: we can work with much higher precision. 

% subsubsection problems_bayes (end)
        
\section{Useful protocols} % (fold)
\label{sec:useful_protocols}

\subsection{Changing the encryption scheme} % (fold)
\label{sub:changing_encryption_scheme}
	Suppose we have two additively homomorphic encryption schemes $\mathsf{E_1}$ and $\mathsf{E_2}$ with the same message space $M$ and two parties Alice and Bob. Alice has the secret keys $sk_1$ and $sk_2$ for both schemes and Bob has a value encrypted with $\mathsf{E_1}$ with the public keys $pk_1$ and $pk_2$. We propose a protocol that enable Bob to get an encryption of its value under $\mathsf{E_2}$ without revealing anything to Alice. In the following, $*$ represents the homomorphic operation over cyphertexts. 
	
	\begin{itemize}
		\item Alice's input: $(sk_1,sk_2)$; Bob's input: $(pk_1,pk_2,c)$ where $c$ is a cyphertext of $\mathsf{E_1}$ under public key $pk_1$.
		   
		\item Bob picks $r \leftarrow M$ uniformly at random, sets $c' = c * \mathsf{E_1.Enc}(r)$ and sends $c'$ to Alice.
		
		\item Alice decrypts $c'$, re-encrypts the result with the new encryption scheme and sends the new cyphertext $c'' = \mathsf{E_2.Enc}(\mathsf{E_1.Dec}(c'))$ to Bob.
		
		\item Bob outputs $\tilde{c} = c'' * \mathsf{E_2.Enc}(r)$  
		
	\end{itemize}
	      
	\paragraph{Security for Alice}
 		We build a simulator that produces an encryption $c''$ of a random element. As we suppose $\mathsf{E_2}$ to be semantically secure, Bob's real view and output (the tuple $(pk_1,pk_2,c,c'',\tilde{c})$) is indistinguishable from the simulated view.

	\paragraph{Security for Bob}
		The simulator sends a random encrypted element $c'$ to Alice which is statistically indistinguishable from the real $c'$ (as both encrypt a value coming from the same uniform distribution over the message space $M$). Thus, as $\mathsf{E_2}$ is semantically secure, the tuple $(sk_1,sk_2,c',\tilde{c})$  generated in the ideal setting is indistinguishable from the real setting.  

% subsection changing_encryption_scheme (end)

% section useful_protocols (end)


\section{Related Work} % (fold)
\label{sec:related_work}
        
\begin{description}
	\item[Secure Multiparty Computational Geometry] (\emph{Atallah, Du}) Some specialized secure MPC protocol for Computational Geometry. Very interesting for us as one of the problem they solve is point inclusion, which is closely related to hyperplane decision. They give protocols to compute dot products based on 1-out-of-N OT, one of which does not use homomorphic encryption.
	\\ \href{https://www.cerias.purdue.edu/assets/pdf/bibtex_archive/2001-48.pdf}{Link} 
	
	\item [On Private Scalar Product Computation for Privacy-Preserving Data Mining](\emph{Goethals, Laur, Lipmaa, Mielikainen})
	\\ \href{http://eprints.pascal-network.org/archive/00000295/01/sspfordm.pdf}{Link} 
	
	\item [Secure Multiparty Computation of Approximations](\emph{Feigenbaum, Ishai, Malkin, Nissim, Strauss, Wright}) Describes a protocol to efficiently approximate the Hamming distance and \#P-hard functions.                                               
	\\ \href{http://cs-www.cs.yale.edu/homes/jf/FIMNSW.pdf}{Link}
	
	\item [ML Confidential:	Machine Learning on Encrypted Data](\emph{Graepel, Lauter, Naehrig}) An implementation, based on FHE, of basic machine learning classification algorithms.	   
	\\ \href{http://research.microsoft.com/pubs/179548/323.pdf}{Link}
	  
	\item [Privacy-Preserving Support Vector Machines Learning](\emph{Zhan,Chang, Matwin}) How to run a learning algorithm on distributed datas in a private way. They essentially show how to use Paillier to obliviously compute dot products.
	\\ \href{http://iceb.nccu.edu.tw/proceedings/2005/477-482.pdf}{Link}
	                                            
	\item [Cryptographically Private Support Vector Machines](\emph{Laur, Lipmaa,Taneli Mielikainen}) Describes private protocol for ``kernel sharing, prediction and training''. Based on Paillier.
	\\ \href{http://eprint.iacr.org/2006/198.pdf}{Link}
	
	\item [Privacy-Preserving SVM using Nonlinear Kernels on Horizontally Partitioned Data] (\emph{Yu, Jiang, Vaidya})
	\\ \href{http://cimic.rutgers.edu/~jsvaidya/pub-papers/vaidyaSVM-sac06.pdf}{Link}
	      
	\item [Privacy-Preserving SVM Classification on Vertically Partitioned Data] (\emph{Yu, Vaidya, Jiang})
	Check the ``Related Work'' section for a small survey of existing protocols.
	\\ \href{http://link.springer.com/content/pdf/10.1007%2F11731139_74.pdf}{Link}
	
	                                   
	\item [Secure Multiparty Computation Goes Live](\emph{Bogetoft et al.}) A large-scale practical implementation of a secure auction using MPC. They describe and prove lots of efficient cryptographic protocols.  
	\\ \href{http://eprint.iacr.org/2008/068.pdf}{Link}
       
	\item [Unconditionally Secure Constant Round Multi-Party Computation for Equality, Comparison, Bits and Exponentiation](\emph{Damgard, Fitzi, Kiltz, Nielsen, Toft})
	\\ \href{http://www.iacr.org/cryptodb/archive/2006/TCC/3624/3624.pdf}{Link}
             
	\item [Oblivious Polynomial Evaluation](\emph{Naor, Pinkas}) How a receiver can evaluate $P(x)$ where $P$ is a polynomial owned by a sender, without disclosing $x$ while preserving the server's secret ($P$).
	\\ \href{http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.73.7197&rep=rep1&type=pdf}{Link}
                                                                          
	\item [Efficient Private Matching and Set Intersection](\emph{Freedman, Nissim, Pinkas}) Various secure MPC protocols to compute set intersection of private datasets.
	\\ \href{http://www.cs.princeton.edu/~mfreed/docs/FNP04-pm.pdf}{Link}
	
	\item [Secure Multi-Party Sorting and Applications](\emph{Jonsson, Kreitz, Uddin})
	\\ \href{http://eprint.iacr.org/2011/122.pdf}{Link}
	                  
	\item [State-of-the-art in Privacy Preserving Data Mining](\emph{Verykios, Bertino,Nai Fovino, Parasiliti Provenza, Saygin, Theodoridis})
	\\ \href{http://www.sigmod.org/publications/sigmod-record/0403/B1.bertion-sigmod-record2.pdf}{Link}
	
	\item [An Efficient Solution to The Millionaires' Problem Based on Homomorphic Encryption](\emph{Lin, Tzeng})
	\\ \href{http://eprint.iacr.org/2005/043.pdf}{Link}
	                
	\item [Secure Two Party Comparison over Encrypted Data](\emph{Kaghazgaran, Sadeghyan})
	\\ \href{http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6141405&tag=1}{Link} 
	
	\item [Practical and Secure Solutions for Integer Comparison](\emph{Gary, Schoenmakers, Villegas})
	Millionaire's protocol for encrypted inputs (the input is given bit by bit).
	\\ \href{http://www.win.tue.nl/~berry/papers/pkc07intcomp.pdf}{Link}   
	
	                                                                                             
	\item [Comparing Encrypted Data](\emph{Veugen})
 	Millionaire's protocol for encrypted inputs (does not entirely fulfill the requirements we need for the second solution of the Bayes classifier, but may be modified to do so).   
	\\ \href{http://msp.ewi.tudelft.nl/sites/default/files/Comparing%20encrypted%20data.pdf}{Link}
\end{description}


% section related_work (end)



\newpage



\bibliography{main}
\bibliographystyle{alpha}





\end{document}
