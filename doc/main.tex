\documentclass[11pt]{article}

\usepackage{fullpage}

\input{commands}

\newcommand{\mysection}[1]{\vspace{3mm}\noindent\textbf{#1} \\}
\DeclareMathOperator*{\argmax}{\mathsf{argmax}}

\title{Notes}

\begin{document}

\date{}

\maketitle

\vspace{-2cm}


\section{Model decision over FHE}

Problem: Two parties have private data and want to learn the result of a function on their data.

\noindent Example use case:  A hospital  has some private medical records of patients and a client wants to learn whether he is likely to get some disease or to get some good treatment from a doctor based on that data. Neither the hospital nor the client want to share their private data.

Discussion:
We realized that using FHE for running the machine learning algorithm is not that useful because the hospital can learn on plaintext data. Instead, what is more interesting is to hide the resulting learned model from the patient (another example is Google flu trends because Google does not want to release its parameters). So we want to perform FHE on the decision algorithm that uses the model to predict the outcome, and even this was non-trivial to figure out.
Using FHE here makes sense because we are going to use it for security benefits rather than performance benefits.


\subsection{Our solution for hyperplane decision surfaces}
\label{sub:hyperplane_decision}      

\newcommand{\res}{\mathsf{res}}

We assume a class of classification models parameterized by a hyperplane $w$
for which the decision function can be expressed as:
\begin{equation*}
  \hat{y}(x) = \text{sgn}(\langle w, x \rangle) 
\end{equation*}

In other words, the server has a vector $w$ (the model) and the client has a
vector $x$ (a particular feature vector). The client wants to know whether
$\langle x, w \rangle > 0$, where $\langle \cdot, \cdot \rangle$ denotes dot product. For
now, we assume that $x$ has a finite dimensional basis (this excludes RBF
surfaces, for example).

The client provides $\enc(x)$ to the server, which can easily compute
$\enc(\langle x, w \rangle)$ by only using 1-depth FHE. 

Note that Paillier suffices for this computation. Even though Paillier does not have multiplication homomorphism, we can still implement
multiplication because the server knows $w$ (the model) in plaintext, and can simply raise a ciphertext to the power of $w_i$ and achieve multiplication by $w_i$. Moreover, Paillier has a large plaintext domain so it can certainly fit the ranges of the classification algorithm. 

Let $\res = \langle x,
w \rangle$.

The challenge is deciding if $\res > 0$. Simply giving the client $\enc(\res)$ leaks $\res$ to the client, which is more information about the model than the final decision.

Instead, the idea is for the server to choose a random $\epsilon \in \bbZ_q$ where $q$ is the space of the plaintext in FHE. The server computes $\enc(\epsilon + \res)$ and the client is able to decrypt this ciphertext and obtain $\epsilon + \res \mod q$, which information-theoretically leaks nothing about $\res$.

Now, ideally we would like the client to compare $\epsilon + \res$ to $\epsilon + q/2$, because $\res > q/2$ indicates a negative $\res$ (that wrapped around). We can use Yao's comparison protocol (the millionaires problem -- a simple one interaction protocol) to decide the result of this comparison without leaking any private information. The challenge though is that the client has $\epsilon + \res \mod q$, and $\mod q$ affects correctness.

The idea is to use a new experiment that has a chance of outputting the correct result $>1/2$, and to repeat this protocol a few times and take majority in order to amplify correctness (which increases exponentially in the number of repeats):

\newcommand{\eps}{\epsilon}

\begin{enumerate}
\item
    The server chooses $\epsilon \in \bbZ_q$ at random. If $\epsilon \geq q/2$,
    let $v = \eps - q/2$. The intuition is that an $\eps \geq q/2$ is likely to
    wrap around when added to another number. If $\eps < q/2$, let $v = \eps +
    q/2$.
\item Run Yao's protocol, where the client's input is $\eps + \res \mod q$ and the server's input is $v$.
\end{enumerate}

We now analyze this protocol to show that the probability of success is greater
than half. Let $r \in \bbZ_q$, and $\epsilon \in \bbZ_q$ be drawn uniformly at
random. We also assume that all $r \in \bbZ_q$ are equally likely.
\rnote{We don't need to assume that r is equally likely. Our correctness holds for all $r$. If we assume all r-s are equally likely, we only prove that our protocol succeeds on barely more than half the possible results, but we can in fact show that it should succeed for all possible results with high probability.}
\rbnote{Assuming that the distribution of $r$ is uniform is also problematic as it would basically imply that the distribution of $x$ is uniform (we can discuss about that). The following proof is not correct anymore when we don't suppose that the distribution is uniform: for example, for the second part of the proof ($r \in \bbZ_q^-$), if the distribution is concentrated around $(q-1)/4$ and that the probability of having $(q-1)/2 < r < q-1$ is small, the last probability can be bigger than $1/2$.}

 For now, we
restrict ourselves to odd $q$. Denote $\bbZ_q^+$ as the integers in $[0,
\frac{q-1}{2}]$, and $\bbZ_q^-$ as the integers in $[\frac{q-1}{2}+1, q)$.

\rnote{once you fix an $r$, you should be able to write the probabilities below in terms of $q-r$ or $r$, for the case when $r>q/2$ and $r<q/2$. }
That means $r \in \bbZ_q^+$ we will consider to encode a number $\geq 0$, and
$r \in \bbZ_q^-$ we will consider to encode a number $< 0$. Let $v$ be defined
as before. We want to show that:
\begin{align*}
    \Pr\left[ (r+\epsilon) \text{ mod } q < v\right]
    \begin{cases}
        > 1/2 & \mbox{if } r\in\bbZ_q^+  \\
        < 1/2 & \mbox{if } r\in\bbZ_q^-
    \end{cases}
\end{align*}
We begin by showing that if $r \in \bbZ_q^+$ and $q \geq 7$, then
$\Pr\left[(r+\epsilon)\text{ mod }q<v\right] > 1/2$:
\begin{align*}
    \Pr\left[(r+\epsilon)\text{ mod }q<v : r \in \bbZ_q^+ \right] &=
        \frac{1}{q}\sum\limits_{i=0}^{q-1}\Pr[(r+\epsilon)\text{ mod }q<v : \epsilon=i, r\in \bbZ_q^+ ] \\
        &= \frac{1}{q}\left[ \sum\limits_{i=0}^{(q-1)/2-1} \frac{(q-1)/2}{(q-1)/2+1} + \sum\limits_{i=(q-1)/2}^{q-1} \frac{i-(q-1)/2}{(q-1)/2+1} \right] \\
        &= \frac{1}{4}\left[3+\frac{1}{q}-\frac{8}{1+q}\right]
\end{align*}


\rnote{the probability statements above and below should be over the random choice of $\epsilon$ rather than $r$}

Notice that this quantity is always $> 1/2$ if $q \geq 7$. We now show the other side, that
if $r \in \bbZ_q^-$, then $\Pr\left[(r+\epsilon)\text{ mod }q<v\right] < 1/2$:
\begin{align*}
    \Pr\left[(r+\epsilon)\text{ mod }q<v:r\in\bbZ_q^-\right] &=
        \frac{1}{q}\sum\limits_{i=0}^{q-1}\Pr[(r+\epsilon)\text{ mod }q<v : \epsilon=i,r\in\bbZ_q^-] \\
        &= \frac{1}{q}\left[ \sum\limits_{i=0}^{(q-1)/2-1} \frac{i}{(q-1)/2} + \sum\limits_{i=(q-1)/2}^{q-1} 0 \right] \\
        &= \frac{q-3}{4q}
\end{align*}
It is clear that for $q \geq 3$, this quantity is $< 1/2$. Since we already
require $q \geq 7$ above, this is ok.

\rnote{as a small nit for style, I (and cryptographers) find this notation more clear : $\Pr[\eps \gets \bbZ_q: (r+\eps) \mod q < v]$ because it is clearer where the randomness comes from and what is the order of steps in the experiment.}

\rnote{based on this probability bound, would be good to estimate how many repetitions we need, for example, using Hoeffding's inequality -- see end of Wikipedia page -- or other inequality you prefer }

\rbnote{Hoeffding's inequality requires the random variables to be independent, which is -- I think -- not the case here: remember that the random variable that we consider is the variable that is zero if the guess is correct, and one is incorrect. We could try to draw from the proof of Goldreich-Levin theorem for this point.}

%if r > q/2
 %if e > q/2 : correct result always
 %if e < q/2 for  q - r < e < q/2 incorrect result -- but for e < q-r it is correct
%each case for e happens with chance 1/2

%if r < q/2 :
%if e < q/2 : correct result always
%if e > q/2: correct result sometimes

\subsection{Another solution for rerandomization and comparison}
           
\subsubsection{Description} 
We propose an other protocol for the hyperplane decision surfaces. The idea is very similar to the one propose in section~\ref{sub:hyperplane_decision}.

The server and the client do the same computations as in~\ref{sub:hyperplane_decision} so the server gets an encrypted version of $r$ under the client secret key. The client wants to know the sign of $v$ and we have to ensure server's security \emph{i.e.} the client can only learn the sign of $r$.

\begin{itemize}
	\item The server choose $\varepsilon \getsr \bbZ_q$ and sends the encryption of $r + \varepsilon \mod q$ to the client who decrypts it.
	\item Run Yao's protocol, where the client's input is $r + \varepsilon$ and the server's input is $\varepsilon$.
\end{itemize} 

In this section, when we work modulo $q$, we consider elements in the interval $(-q/2, q/2]$. The client gets the right sign of $r$ if and only if $r + \varepsilon$ does not wrap \emph{i.e.}
\[
	-q/2 < r + \varepsilon \leq q/2 \Leftrightarrow  -q/2 - r< \varepsilon \leq q/2 - r
\]

As a consequence, given $r$, the probability that the protocol is correct is 
\[ 
	\Pr[\varepsilon \getsr \bbZ_q\text{ : Protocol is correct} | r] = \frac{q - |r|}{q} = 1 - \frac{|r|}{q}
\]
and the probability that the client gets the correct sign when $r$ is distributed according to $\mathcal{D}$ is 
\begin{align*}
	\Pr[\varepsilon \getsr \bbZ_q, r \gets \mathcal{D}\text{ : Protocol is correct}] 
		&= \sum_{r \in \mathcal{D}} \Pr[r] (1 - \frac{|r|}{q}) \\
		&= 1 - \frac{1}{q}\sum_{r \in \mathcal{D}} |v| \Pr[v]  \\
		&= 1 - \frac{\mathbb{E}(|\mathcal{D}|)}{q}
\end{align*}
In particular, if $\mathcal{D}$ is bounded by $B$ ($\Pr[ r \gets \mathcal{D} : |r| > B]$), we have
\[
\Pr[\varepsilon \getsr \bbZ_q, r \gets \mathcal{D}\text{ : Protocol is correct}] \geq 1 - \frac{B}{q}
\] 
    
\subsubsection{Repeating the experiment}

We want to repeat the previous protocol $k$ times in order to decrease the probability of a wrong guess: we run the protocol $k$ times and take as an output the majority of outputs. 

Let $X_i$ be the random variable that is 0 if the output of the $i$-th execution of the protocol is correct, and that is 1 otherwise. The final guess is correct if $\sum_{i=1}^k X_i > \frac{k}{2}$. Let $r$ be a possible output of $\mathcal{D}$. For a fixed $r$, it is easy to see that the $X_i$ are independent random variables. Thus, we can apply Chernov's bound:

\[
    \forall \delta > 0,  \Pr\left[ \sum_{i=0}^k X_i \geq (1 + \delta)\mu\right] \leq \left( \frac{e^\delta}{(1+\delta)^{1+\delta}} \right)^\mu
\]
where $\mu = \sum \mathbb{E}(X_i)$. Here, $\mu = k \frac{|r|}{q}$ (remember we fixed $r$). We assumed before that $|r| < q$ and WLOG we can also assume that $|r| < q/2$. And as we want that  $\sum_{i=1}^k X_i > \frac{k}{2}$ with high probability, we can take $\delta = \frac{q}{2|r|} -1 > 0$. We infer that, for $r \neq 0$,
\[
	\Pr[\text{Protocol is not correct} | r] \leq \left(\frac{e^{\frac{q}{2|r|} -1}}{\frac{q}{2|r|}^{\frac{q}{2|r|}}}\right)^{k \frac{|r|}{q}} 
	\leq \left(\frac{2e}{q}\right)^{\frac{k}{2}}
\]
(if $r = 0$, one iteration of the protocol always succeeds as $r + \varepsilon$ does not wrap).


However, repeating the experiment, leaks some informations about $r$: $\mathbb{E}(\sum X_i | v) = k \frac{|r|}{q}$ meaning that the number of incorrect experiments -- and as consequence, if we repeated a sufficient number of times, the number of experiments whose outcome is different from the final outcome -- is about $k \frac{|r|}{q}$. Thus, the more we repeat the protocol, the best we can approximate $|r|$.

\rbnote{We must choose the parameters carefully if we do not want to disclose to many informations about $r$.}

\rbnote{We should also have this issue when we repeat the previous protocol.}


\subsection{Na\"ive Bayes solution}

%For na\"ive Bayes, we need to convert the table of values into a matrix so we can FHE multiply it to the input $x$. TODO: we need to figure out how to compute the maximum of $k$ values over FHE, likely using the idea above which computes $\max$ of $q/2$ and $\res$.
%
\newcommand{\NB}{Na\"ive Bayes}

We now consider categorial \NB{}. Suppose we have $k$ classes and $n$ features.
Then recall that a \NB{} classifier is parameterized by $\theta_{\mathcal{C}} =
\{\theta_{c}=p(\mathcal{C}=c)\}_{c=1}^{k}$ and $\theta_{\mathcal{F}|c} =
\{\theta_{f|c}= \{ p(\mathcal{F}_f=x|c) \}_{x=1}^{|\mathcal{F}_f|}
\}_{f=1}^{n}$ for each $c = 1,...,k$. The decision function is simply:
\begin{equation*}
  \hat{y}(x) = \argmax_{c} p(c, x) = p(c)\prod\limits_{i=1}^{n}p(x_i|c)
\end{equation*}
To make this easier for FHE eval, we assume that the client supplies us with each feature value
in $1$-of-$k$ encoding, so we receive $n$ ciphertexts $(\enc(x_1), ..., \enc(x_n))$. So now we
can express the decision function as a product of dot products:
\begin{equation*}
  \hat{y}(x) = \argmax_{c} p(c, x) = p(c)\prod\limits_{i=1}^{n} \langle \theta_{\mathcal{F}_i|c}, x_i \rangle
\end{equation*}

$\max$ is tricky and we currently do not know how to do it.
      


\section{Related Work} % (fold)
\label{sec:related_work}
        
\begin{description}
	\item[Secure Multiparty Computational Geometry] (\emph{Atallah, Du}) Some specialized secure MPC protocol for Computational Geometry. Very interesting for us as one of the problem they solve is point inclusion, which is closely related to hyperplane decision. They give protocols to compute dot products based on 1-out-of-N OT, one of which does not use homomorphic encryption.
	\\ \href{https://www.cerias.purdue.edu/assets/pdf/bibtex_archive/2001-48.pdf}{Link}
	
	\item [Secure Multiparty Computation of Approximations](\emph{Feigenbaum, Ishai, Malkin, Nissim, Strauss, Wright}) Describes a protocol to efficiently approximate the Hamming distance and \#P-hard functions.                                               
	\\ \href{http://cs-www.cs.yale.edu/homes/jf/FIMNSW.pdf}{Link}
	
	\item [ML Confidential:	Machine Learning on Encrypted Data](\emph{Graepel, Lauter, Naehrig}) An implementation, based on FHE, of basic machine learning classification algorithms.	   
	\\ \href{http://research.microsoft.com/pubs/179548/323.pdf}{Link}
	                                                                   
	\item [Secure Multiparty Computation Goes Live](\emph{Bogetoft et al.}) A large-scale practical implementation of a secure auction using MPC. They describe and prove lots of efficient cryptographic protocols.  
	\\ \href{http://eprint.iacr.org/2008/068.pdf}{Link}
       
	\item [Unconditionally Secure Constant Round Multi-Party Computation for Equality, Comparison, Bits and Exponentiation](\emph{Damgard, Fitzi, Kiltz, Nielsen, Toft})
	\\ \href{http://www.iacr.org/cryptodb/archive/2006/TCC/3624/3624.pdf}{Link}
             
	\item [Oblivious Polynomial Evaluation](\emph{Naor, Pinkas}) How a receiver can evaluate $P(x)$ where $P$ is a polynomial owned by a sender, without disclosing $x$ while preserving the server's secret ($P$).
	\\ \href{http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.73.7197&rep=rep1&type=pdf}{Link}
                                                                          
	\item [Efficient Private Matching and Set Intersection](\emph{Freedman, Nissim, Pinkas}) Various secure MPC protocols to compute set intersection of private datasets.
	\\ \href{http://www.cs.princeton.edu/~mfreed/docs/FNP04-pm.pdf}{Link}
	
	\item [Secure Multi-Party Sorting and Applications](\emph{Jonsson, Kreitz, Uddin})
	\\ \href{http://eprint.iacr.org/2011/122.pdf}{Link}
	
\end{description}


% section related_work (end)



\newpage



\bibliography{main}
\bibliographystyle{alpha}





\end{document}
